# Server Configuration
PORT=3000

# Local LLM Configuration (optional)
# If you have Ollama installed locally, you can use local models
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=smollm